{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "   - Simple Linear Regression is a statistical technique that models the relationship between a dependent variable and one independent variable using a straight line. The line is calculated to minimize the difference between the observed and predicted values.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "   - The key assumptions of Simple Linear Regression include a linear relationship between the variables, independence of residuals, homoscedasticity (constant variance of residuals), normally distributed residuals, and the absence of significant outliers.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "   - In the equation Y = mX + c, the coefficient m represents the slope of the regression line. It indicates how much the dependent variable Y changes for each one-unit increase in the independent variable X.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "   - The intercept c in the equation Y = mX + c is the value of Y when X equals zero. It provides the point where the regression line crosses the Y-axis and represents the starting value of the prediction.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "   - The slope m is calculated using the formula: m = Σ(Xi - X̄)(Yi - Ȳ) / Σ(Xi - X̄)². This formula calculates the rate of change in Y with respect to X by dividing the covariance of X and Y by the variance of X.\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "   - The purpose of the least squares method is to minimize the sum of the squared differences between observed and predicted values. This method ensures that the regression line fits the data as closely as possible.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "   - The coefficient of determination (R²) indicates the proportion of the variance in the dependent variable that is explained by the model. An R² of 0.9 means that 90% of the variance in Y is explained by X.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "   - Multiple Linear Regression is an extension of simple linear regression that models the relationship between a dependent variable and two or more independent variables. It allows the model to consider the combined effect of multiple predictors.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "   - The main difference between Simple and Multiple Linear Regression is that the former uses one independent variable to predict the outcome, while the latter uses two or more independent variables.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "   - The key assumptions of Multiple Linear Regression are linearity of relationships, independence of residuals, homoscedasticity, normally distributed residuals, and no multicollinearity among independent variables.\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "   - Heteroscedasticity occurs when the variance of residuals is not constant across all levels of the independent variables. It can distort the accuracy of coefficient estimates and lead to incorrect statistical inferences.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "   - To improve a model with high multicollinearity, one can remove or combine correlated variables, use dimensionality reduction techniques like principal component analysis, or apply regularization methods such as Ridge or Lasso regression.\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "   - Categorical variables can be transformed using encoding methods like one-hot encoding, which creates separate binary columns for each category, or label encoding, which assigns numerical values to categories.\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "   - Interaction terms are used in regression to capture the effect of two or more variables acting together. They help in understanding how the relationship between one independent variable and the dependent variable changes depending on the level of another variable.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "   - The interpretation of the intercept differs in the two models because in Simple Linear Regression it represents the value of Y when X is zero, while in Multiple Linear Regression, it represents the value of Y when all independent variables are zero, which may not always be meaningful.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "   - The slope in regression analysis represents the influence of an independent variable on the dependent variable. A larger slope means a stronger relationship, and the sign of the slope (positive or negative) indicates the direction of the relationship.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "   - The intercept provides context by anchoring the regression line. It shows the baseline level of the dependent variable when all predictors are zero, helping interpret how much each predictor shifts the outcome from this baseline.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "   - The limitations of using R² alone include its inability to detect overfitting, its tendency to increase with more predictors regardless of relevance, and its failure to account for model complexity or residual patterns.\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "   - A large standard error for a regression coefficient indicates that the estimate is not precise, suggesting that the corresponding predictor may not be statistically significant or that the data has high variability.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "   - In residual plots, heteroscedasticity is identified by patterns such as a fan or cone shape. It is important to address because it violates the assumption of constant variance and can make confidence intervals and p-values unreliable.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "   - If a Multiple Linear Regression model has high R² but low adjusted R², it suggests that some of the predictors are not contributing meaningful information and the model may be overfitting the training data.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "   - Scaling variables is important in Multiple Linear Regression because variables on different scales can disproportionately influence the model. Scaling ensures that each variable contributes equally to the analysis, especially when using regularization.\n",
        "    \n",
        "23. What is polynomial regression?\n",
        "   - Polynomial regression is a form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial to capture non-linear trends.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "   - Polynomial regression differs from linear regression in that it fits a curved line instead of a straight one, making it suitable for modeling more complex relationships where changes in Y are not proportional to changes in X.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "   - Polynomial regression is used when data shows a non-linear pattern that a straight line cannot capture effectively, such as in cases of acceleration, decay, or seasonal growth trends.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "   - The general equation for polynomial regression is Y = b0 + b1X + b2X² + b3X³ + ... + bnXⁿ, where n is the degree of the polynomial and the coefficients represent the weights for each term.\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "   - Polynomial regression can be extended to include multiple variables by incorporating polynomial and interaction terms for each predictor. This creates a multivariate polynomial regression model.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "   - The limitations of polynomial regression include the risk of overfitting with high-degree polynomials, increased sensitivity to outliers, and reduced interpretability as the number of terms increases.\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "   - Methods to evaluate model fit when selecting the degree of a polynomial include adjusted R², cross-validation, and information criteria like AIC or BIC. Visual inspection of residual plots is also useful.\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "   - Visualization is important in polynomial regression because it helps understand how well the curve fits the data, reveals overfitting or underfitting, and makes the model’s behavior more interpretable to humans.\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "   - Polynomial regression in Python can be implemented using the PolynomialFeatures class from sklearn.preprocessing to generate polynomial terms, and then applying LinearRegression from sklearn.linear_model to fit the model.\n"
      ],
      "metadata": {
        "id": "9ZbyMFlMZbAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emnz6gCGY48H"
      },
      "outputs": [],
      "source": []
    }
  ]
}